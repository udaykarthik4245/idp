{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1mZGdcXwzGLW5PJYJl0WF1nSqrxcJDYI6",
      "authorship_tag": "ABX9TyMyKoOkLTNHtpMSz7Au9mt3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/udaykarthik4245/idp/blob/main/VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCOYIu_6OxZ4",
        "outputId": "2b71065a-b6b1-42e4-c235-de22f456a318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "280/280 [==============================] - 66s 182ms/step - loss: 3.5483 - accuracy: 0.0295\n",
            "Epoch 2/10\n",
            "280/280 [==============================] - 53s 188ms/step - loss: 3.3337 - accuracy: 0.0312\n",
            "Epoch 3/10\n",
            "280/280 [==============================] - 55s 195ms/step - loss: 3.3336 - accuracy: 0.0330\n",
            "Epoch 4/10\n",
            "280/280 [==============================] - 53s 191ms/step - loss: 3.3327 - accuracy: 0.0335\n",
            "Epoch 5/10\n",
            "280/280 [==============================] - 54s 191ms/step - loss: 3.3326 - accuracy: 0.0281\n",
            "Epoch 6/10\n",
            "280/280 [==============================] - 54s 191ms/step - loss: 3.3327 - accuracy: 0.0304\n",
            "Epoch 7/10\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "IMAGE_SIZE = (224, 224)\n",
        "NUM_CLASSES = 28\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# Define path to train folder\n",
        "train_data_dir = '/content/drive/MyDrive/resize dataset/training'\n",
        "\n",
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for class_label, class_name in enumerate(sorted(os.listdir(folder))):\n",
        "        class_folder = os.path.join(folder, class_name)\n",
        "        for filename in os.listdir(class_folder):\n",
        "            img = load_img(os.path.join(class_folder, filename), target_size=IMAGE_SIZE)\n",
        "            img_array = img_to_array(img)\n",
        "            images.append(img_array)\n",
        "            labels.append(class_label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "x_train, y_train = load_images_from_folder(train_data_dir)\n",
        "\n",
        "x_train = x_train.astype('float32') / 255\n",
        "y_train = to_categorical(y_train, NUM_CLASSES)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Block 1\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "# Block 2\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "# Block 3\n",
        "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "# Block 4\n",
        "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "# Block 5\n",
        "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "\n",
        "opt = Adam(learning_rate=0.001, beta_1=0.99, beta_2=0.999)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model and display the training accuracy for each epoch\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS}')\n",
        "    model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=1, verbose=1)\n",
        "\n",
        "# Display the overall training accuracy\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "print(f'Overall training accuracy: {train_acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG16"
      ],
      "metadata": {
        "id": "6XisrA5RXFR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#trained model\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "IMAGE_SIZE = (224, 224)\n",
        "NUM_CLASSES = 28\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# Define path to train folder\n",
        "train_data_dir = '/content/drive/MyDrive/resize dataset/training'\n",
        "\n",
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for class_label, class_name in enumerate(sorted(os.listdir(folder))):\n",
        "        class_folder = os.path.join(folder, class_name)\n",
        "        for filename in os.listdir(class_folder):\n",
        "            img = load_img(os.path.join(class_folder, filename), target_size=IMAGE_SIZE)\n",
        "            img_array = img_to_array(img)\n",
        "            images.append(img_array)\n",
        "            labels.append(class_label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load and preprocess data\n",
        "x_train, y_train = load_images_from_folder(train_data_dir)\n",
        "x_train = x_train.astype('float32') / 255\n",
        "y_train = to_categorical(y_train, NUM_CLASSES)\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Use VGG16 as the backbone and fine-tune it\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "\n",
        "# Freeze the weights of the pre-trained layers\n",
        "base_model.trainable = False\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)  # Adjust learning rate\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with data augmentation\n",
        "model.fit(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
        "          steps_per_epoch=len(x_train) / BATCH_SIZE, epochs=NUM_EPOCHS)\n",
        "\n",
        "# Evaluate the model\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "print(f'Overall training accuracy: {train_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mQHIgKOO-c5",
        "outputId": "f4b7ef04-23d1-44b0-e4dd-1f23b1e9f00d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 1s 0us/step\n",
            "Epoch 1/10\n",
            "280/280 [==============================] - 38s 106ms/step - loss: 2.3306 - accuracy: 0.3531\n",
            "Epoch 2/10\n",
            "280/280 [==============================] - 31s 111ms/step - loss: 0.7080 - accuracy: 0.8054\n",
            "Epoch 3/10\n",
            "280/280 [==============================] - 31s 111ms/step - loss: 0.3138 - accuracy: 0.9156\n",
            "Epoch 4/10\n",
            "280/280 [==============================] - 31s 109ms/step - loss: 0.1896 - accuracy: 0.9455\n",
            "Epoch 5/10\n",
            "280/280 [==============================] - 30s 108ms/step - loss: 0.1312 - accuracy: 0.9661\n",
            "Epoch 6/10\n",
            "280/280 [==============================] - 30s 107ms/step - loss: 0.1056 - accuracy: 0.9723\n",
            "Epoch 7/10\n",
            "280/280 [==============================] - 31s 110ms/step - loss: 0.0898 - accuracy: 0.9746\n",
            "Epoch 8/10\n",
            "280/280 [==============================] - 31s 111ms/step - loss: 0.0794 - accuracy: 0.9777\n",
            "Epoch 9/10\n",
            "280/280 [==============================] - 31s 109ms/step - loss: 0.0666 - accuracy: 0.9795\n",
            "Epoch 10/10\n",
            "280/280 [==============================] - 30s 108ms/step - loss: 0.0776 - accuracy: 0.9746\n",
            "Overall training accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using one dense layer"
      ],
      "metadata": {
        "id": "Fx_Scni-a0b5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#trained model\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "IMAGE_SIZE = (224, 224)\n",
        "NUM_CLASSES = 28\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# Define path to train folder\n",
        "train_data_dir = '/content/drive/MyDrive/resize dataset/training'\n",
        "\n",
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for class_label, class_name in enumerate(sorted(os.listdir(folder))):\n",
        "        class_folder = os.path.join(folder, class_name)\n",
        "        for filename in os.listdir(class_folder):\n",
        "            img = load_img(os.path.join(class_folder, filename), target_size=IMAGE_SIZE)\n",
        "            img_array = img_to_array(img)\n",
        "            images.append(img_array)\n",
        "            labels.append(class_label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load and preprocess data\n",
        "x_train, y_train = load_images_from_folder(train_data_dir)\n",
        "x_train = x_train.astype('float32') / 255\n",
        "y_train = to_categorical(y_train, NUM_CLASSES)\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Use VGG16 as the backbone and fine-tune it\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(Flatten())\n",
        "# model.add(Dense(256, activation='relu'))\n",
        "\n",
        "model.add(Dense(512, activation='relu'))\n",
        "# model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "\n",
        "# Freeze the weights of the pre-trained layers\n",
        "base_model.trainable = False\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)  # Adjust learning rate\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with data augmentation\n",
        "model.fit(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
        "          steps_per_epoch=len(x_train) / BATCH_SIZE, epochs=NUM_EPOCHS)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uJyOmCkY479",
        "outputId": "6cf6d9e2-24c0-474c-c858-947060a8769a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "280/280 [==============================] - 32s 110ms/step - loss: 1.9133 - accuracy: 0.5228\n",
            "Epoch 2/10\n",
            "280/280 [==============================] - 30s 107ms/step - loss: 0.6718 - accuracy: 0.8638\n",
            "Epoch 3/10\n",
            "280/280 [==============================] - 30s 108ms/step - loss: 0.3645 - accuracy: 0.9295\n",
            "Epoch 4/10\n",
            "280/280 [==============================] - 30s 108ms/step - loss: 0.2506 - accuracy: 0.9545\n",
            "Epoch 5/10\n",
            "280/280 [==============================] - 31s 112ms/step - loss: 0.1665 - accuracy: 0.9781\n",
            "Epoch 6/10\n",
            "280/280 [==============================] - 31s 111ms/step - loss: 0.1280 - accuracy: 0.9821\n",
            "Epoch 7/10\n",
            "280/280 [==============================] - 30s 108ms/step - loss: 0.0963 - accuracy: 0.9866\n",
            "Epoch 8/10\n",
            "280/280 [==============================] - 30s 109ms/step - loss: 0.0917 - accuracy: 0.9835\n",
            "Epoch 9/10\n",
            "280/280 [==============================] - 32s 113ms/step - loss: 0.0712 - accuracy: 0.9884\n",
            "Epoch 10/10\n",
            "280/280 [==============================] - 31s 111ms/step - loss: 0.0602 - accuracy: 0.9902\n",
            "Overall training accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate the model\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "print(\"Overall training accuracy:\" ,train_acc*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_M9caTOcrl5",
        "outputId": "5b573489-1514-42c1-9c5a-e7a064cc609e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall training accuracy: 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path to test folder\n",
        "test_data_dir = '/content/drive/MyDrive/resize dataset/testing'\n",
        "\n",
        "# Load and preprocess test data\n",
        "x_test, y_test = load_images_from_folder(test_data_dir)\n",
        "x_test = x_test.astype('float32') / 255\n",
        "y_test = to_categorical(y_test, NUM_CLASSES)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "_, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "# print(f'Overall test accuracy: {test_acc:.4f}')\n",
        "print(\"test accuracy\",test_acc)\n",
        "print(test_acc*100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klPV6nqoa3C1",
        "outputId": "e9d20677-b02a-47fa-8375-55d3ee971e53"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy 0.9928571581840515\n",
            "99.28571581840515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "after batch normalization"
      ],
      "metadata": {
        "id": "pdDichBOs0qH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
        "\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "\n",
        "# Add batch normalization after the base model\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add dropout layer\n",
        "model.add(Dropout(0.5))  # You can adjust the dropout rate as needed\n",
        "\n",
        "model.add(Dense(512, activation='relu'))\n",
        "\n",
        "# Add batch normalization after the dense layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "\n",
        "# Freeze the weights of the pre-trained layers\n",
        "base_model.trainable = False\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with data augmentation\n",
        "model.fit(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
        "          steps_per_epoch=len(x_train) / BATCH_SIZE, epochs=NUM_EPOCHS)\n",
        "\n",
        "# Evaluate the model\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "print(f'Overall training accuracy: {train_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4Ln6C-XZ-nP",
        "outputId": "e1e62b34-87d8-4814-ab0c-f04b4dd386ba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "280/280 [==============================] - 34s 109ms/step - loss: 1.4016 - accuracy: 0.6237\n",
            "Epoch 2/10\n",
            "280/280 [==============================] - 31s 111ms/step - loss: 0.3943 - accuracy: 0.8996\n",
            "Epoch 3/10\n",
            "280/280 [==============================] - 32s 113ms/step - loss: 0.2422 - accuracy: 0.9295\n",
            "Epoch 4/10\n",
            "280/280 [==============================] - 30s 108ms/step - loss: 0.1648 - accuracy: 0.9558\n",
            "Epoch 5/10\n",
            "280/280 [==============================] - 30s 109ms/step - loss: 0.1417 - accuracy: 0.9612\n",
            "Epoch 6/10\n",
            "280/280 [==============================] - 31s 110ms/step - loss: 0.1064 - accuracy: 0.9746\n",
            "Epoch 7/10\n",
            "280/280 [==============================] - 31s 109ms/step - loss: 0.0944 - accuracy: 0.9781\n",
            "Epoch 8/10\n",
            "280/280 [==============================] - 30s 108ms/step - loss: 0.0995 - accuracy: 0.9723\n",
            "Epoch 9/10\n",
            "280/280 [==============================] - 30s 107ms/step - loss: 0.0833 - accuracy: 0.9781\n",
            "Epoch 10/10\n",
            "280/280 [==============================] - 31s 111ms/step - loss: 0.0638 - accuracy: 0.9812\n",
            "Overall training accuracy: 1.0000\n"
          ]
        }
      ]
    }
  ]
}